# Setup for Linux WSL runtime on windows
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks:
      - local_net
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    deploy:
      restart_policy:
        condition: unless-stopped
    environment:
      # GPU Configuration
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Network Configuration
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434

      # Performance Optimizations for RTX 5080
#      - OLLAMA_NUM_PARALLEL=1 # Handle 4 parallel requests (adjust based on VRAM)
      - OLLAMA_MAX_LOADED_MODELS=1 # Keep 2 models in VRAM simultaneously
      - OLLAMA_KEEP_ALIVE=30m # Keep models loaded for 30 minutes

      # Context Window (RTX 5080 has plenty of VRAM)
      - OLLAMA_NUM_CTX=32768 # 32K context window (128K for supported models)

      # GPU Acceleration
      - OLLAMA_NUM_GPU=999 # Use all available GPU layers
      - OLLAMA_GPU_OVERHEAD=0 # Minimize GPU memory overhead

      # Flash Attention (if supported by model)
      - OLLAMA_FLASH_ATTENTION=1 # Enable flash attention for faster inference

      # Memory Management
#      - OLLAMA_MAX_VRAM=0 # 0 = use all available VRAM

      # Optional: Batch size for processing
      - OLLAMA_NUM_BATCH=512 # Larger batch size for RTX 5080

    healthcheck:
      test: ["CMD", "ollama", "--version"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    platform: linux/amd64
    extra_hosts:
      - "ollama:host-gateway"
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    networks:
      - local_net
    # Host port 8081 maps to container port 8080
    ports:
      - "8081:8080"
    volumes:
      # ðŸ”‘ CRITICAL WSL CHANGE: C:/... must become /c/... for WSL volume binding
      - ./data/openwebui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_NAME=OpenWebUI
      - WEBUI_VERSION=latest
      - RAG_ENABLED=true # Enable RAG
      - RAG_MODEL=sentence-transformers/all-mpnet-base-v2 # Popular embedding model
      - RAG_INDEX=faiss # Use FAISS for indexing
      - RAG_TOP_K=7 # Retrieve top 5 documents
      - SEARCH_URL=http://searxng:8080/search?q=<query>
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: unless-stopped
    platform: linux/amd64
    extra_hosts:
      - "ollama:host-gateway"

  glance:
    container_name: glance
    image: glanceapp/glance
    restart: unless-stopped
    # depends_on:
    #   - local_api
    volumes:
      - ./glance/config:/app/config
      - ./glance/assets:/app/assets
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # - ./certs:/certs:ro
    ports:
      # - 8443:8443
      - 8090:8080
    env_file: .env
    # environment:
    #   GLANCE_ENABLE_TLS: "true"
    #   GLANCE_TLS_CERT: /certs/localhost.crt
    #   GLANCE_TLS_KEY: /certs/localhost.key
    labels:
      glance.name: Glance
      glance.icon: sh:glance
      glance.url: http://localhost:8090
      glance.description: Home Dashboard
  # local_api:
  #   container_name: glance-local-api
  #   build:
  #     context: .
  #     dockerfile: glance/custom_api_extension/Dockerfile
  #   restart: unless-stopped
  #   privileged: true
  #   networks:
  #     - local_net
  #   ports:
  #     - "5001:5001"
  #   env_file: .env
  #   environment:
  #     - FLASK_ENV=production
  #   volumes:
  #     - /etc/localtime:/etc/localtime:ro
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "python",
  #         "-c",
  #         "import urllib.request; urllib.request.urlopen('http://localhost:5001/')",
  #       ]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 10s
  #   labels:
  #     glance.name: Server Control
  #     glance.icon: mdi:server-outline
  #     glance.url: http://localhost:5001
  #     glance.description: Host shutdown/restart API
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    networks:
      - local_net
    ports:
      - "8082:8080"
    volumes:
      - ./config/searxng-config:/etc/searxng
      - ./data/searxng-data:/var/lib/searxng
    environment:
      - SEARXNG_BASE_URL=http://searxng:8082
    depends_on:
      - ollama
networks:
  local_net:
    driver: bridge
